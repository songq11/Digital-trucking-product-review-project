{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff467ed-7c9b-4931-9541-817f3eda838c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook tries several tokenization methods for Chinese language to prepare for NLP analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ff7b1c-146f-4c47-bf58-1152039d2f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_parquet('../processed_data/clean_data_final.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073c4910-4bda-49f1-972b-08aed4d361ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More data cleaning: replace 'p子'，'\\xa0', \\u3000\n",
    "## Replace \"p子\" with \"骗子\"\n",
    "mapping = {'p子': '骗子',\n",
    "          'p局': '骗局'}\n",
    "df['fulltext'] = df['fulltext'].replace(mapping, regex=True)\n",
    "\n",
    "drop_words = r'\\xa0|\\u3000'\n",
    "df['fulltext'] = df['fulltext'].replace(drop_words, '', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f4261b-bcd4-4f50-87e2-23de42b02df2",
   "metadata": {},
   "source": [
    "# Tokenization using HanLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0526003-8b28-48c4-9aad-831b0725e4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing Data with HanLP\n",
    "import hanlp\n",
    "hanlp.pretrained.tok.ALL \n",
    "hanlp.pretrained.tok.ALL.keys() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d62f532-fea9-425d-a5ff-a48ed0667487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "tok = hanlp.load(hanlp.pretrained.tok.COARSE_ELECTRA_SMALL_ZH)\n",
    "tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73381825-8b4f-4867-b43a-213cb9702b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(150,161):\n",
    "    print(df.iloc[i]['description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa6b3c7-ab3b-469e-afdc-c13211891d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using pipeline to split sentences and words\n",
    "HanLP = hanlp.pipeline() \\\n",
    "    .append(hanlp.utils.rules.split_sentence) \\\n",
    "    .append(tok)\n",
    "HanLP.append(lambda sents: sum(sents, []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08b873f-a3ed-4516-90d1-5f52056e0d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5,11):\n",
    "    print(HanLP(df.iloc[i]['description']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57699869-ba55-4ce3-82b5-b57765133ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not using pipeline\n",
    "for i in range(1,5):\n",
    "    print(tok(df.iloc[i]['fulltext']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865ceeef-60b2-4324-8102-c191478731e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test CTB9_TOK_ELECTRA_BASE tokenization model\n",
    "tok_test_1 = hanlp.load(hanlp.pretrained.tok.CTB9_TOK_ELECTRA_BASE)\n",
    "for i in range(1,5):\n",
    "    print(tok_test_1(df.iloc[i]['fulltext']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a8d946-e3fb-445b-89dd-70a0f139348c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CTB9_TOK_ELECTRA_BASE seems to work better than the COARSE one, so I will this the result of tok_test_1 for me.\n",
    "# To-do:\n",
    "# Compare with other tokenization models and select the best one.\n",
    "# Need to learn how to determine which works the best. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a813875c-186d-4526-b403-7c39e4610785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rewrite the pipeline of tok_test_1\n",
    "HanLP_1 = hanlp.pipeline() \\\n",
    "    .append(hanlp.utils.rules.split_sentence) \\\n",
    "    .append(tok_test_1)\n",
    "HanLP.append(lambda sents: sum(sents, []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d185146-1438-4bd8-ace7-98e8ecc4defd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tok_title'] = df['title']\n",
    "dst_idx = df.columns.get_loc('tok_title')\n",
    "src_idx = df.columns.get_loc('title')\n",
    "from tqdm.notebook import tqdm\n",
    "for i in tqdm(range(len(df))):\n",
    "    try:\n",
    "        title = df.iat[i, src_idx]\n",
    "        df.iat[i, dst_idx] = HanLP_1(title)\n",
    "    except:\n",
    "        print(i, title)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf85d9a8-c123-4c14-b653-0f4a6881a513",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(150,161):\n",
    "    print(df.iloc[i]['tok_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992f3d34-3f10-4c61-9148-d0ba71d7e54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize \"fulltext\" with tok_test_1\n",
    "# The pipeline generates a list of list in each row, which is not desired, not using pipeline now.\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "df['tok'] = df['fulltext'].progress_apply(tok_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43acec54-deb0-455b-93bd-b6909bee5455",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns = {'tok': 'han_tok'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0fc1d9-17a2-4291-a264-ad21141de448",
   "metadata": {},
   "source": [
    "# Tokenization using Jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cef4bf3-c3e8-4f8e-a8b2-dd8c71236c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195e89d3-2666-4a59-89d0-97181ea894f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding=utf-8\n",
    "import jieba\n",
    "\n",
    "def jieba_tok(text):\n",
    "    return list(jieba.cut(text))\n",
    "\n",
    "df['jieba_tok'] = df['fulltext'].apply(jieba_tok)\n",
    "df['jieba_tok'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158407b4-7817-48bf-8c4b-4fdf5c805bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['fulltext'].str.contains('货拉拉')]['fulltext']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ce8e07-eab0-4acd-a15d-667ec7323dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "jieba.add_word('运满满')\n",
    "jieba.add_word('货车帮')\n",
    "jieba.add_word('满帮')\n",
    "jieba.add_word('货拉拉')\n",
    "\n",
    "df['jieba_tok'] = df['fulltext'].apply(jieba_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff01310-dd5a-44a9-88a4-c83fb2db7601",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['jieba_tok'].head() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751af249-5558-48e8-98c9-0543c09b166b",
   "metadata": {},
   "source": [
    "# Remove stopwords, punctuation, and other noises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d189ffca-92cd-48fb-8da3-dfac3e309b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Document/cn_stopwords.txt', 'r') as file:\n",
    "    stopwords = [line.strip() for line in file]\n",
    "    print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e85789-9f70-4c94-8785-c2e7b5328cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "punctuation = list(string.punctuation)\n",
    "\n",
    "import re\n",
    "pattern = r'[é《》（）〔〕.,·+？！()/\\%％。，、；;&\"“”：:【】=—-丨~－一～─┤Ｔ*‘’!#$§<>?@\\_`{|}﹝﹞¨©°À×÷ˉΣαβπФ‐–―‘’…‰※ⅡⅢⅰⅱⅲⅳⅴⅵ←↑→↓√∥≠≦≧⊙①②③④⑤⑥⑦⑧⑨⑩⑴⑵⒋┤╱■□▲►◆◇○◎●★☟〇〈〉「」『-]'\n",
    "sign = list(pattern)\n",
    "\n",
    "noises = stopwords + punctuation + sign\n",
    "\n",
    "import re\n",
    "number_re = re.compile(r'^[0-9]*(\\.)?[0-9]*$')\n",
    "assert number_re.match('1.2')\n",
    "assert not number_re.match('1.2x')\n",
    "\n",
    "def remove_noises(word_list):\n",
    "    filtered_words = []\n",
    "    for word in word_list:\n",
    "        if word not in noises and not number_re.match(word):\n",
    "            filtered_words.append(word)\n",
    "    return filtered_words\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "df['clean_tok'] = df['jieba_tok'].progress_apply(remove_noises)\n",
    "any(i is None for i in df['clean_tok'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76725e3e-2756-446a-bf8a-68b18547981d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove more chaos by checking word frequency \n",
    "words_des = sum(df['clean_tok'], [])\n",
    "from collections import Counter\n",
    "ct = Counter(words_des)\n",
    "ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5681eb-c17d-468e-a3ea-25d03968e1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "chaos = ['【', '】', '...', '）', '（', '#', '[', ']', '！！！', ',', '——', '-', '！！', '*', '.', '……', '+',\n",
    "        '!', ':', '/', '--', '|', '(', '~', '…', '—', ')', '～', '？？？', ';', '·', '？？', '。。。', '。。'\n",
    "        '！！！！', '---', '「', '」', 'quot', '..', '●', '**' ]\n",
    "\n",
    "def remove_chaos(word_list):\n",
    "    clean_words = []\n",
    "    for word in word_list:\n",
    "        if word not in chaos:\n",
    "            clean_words.append(word)\n",
    "    return clean_words\n",
    "\n",
    "df['filtered_title'] = df['filtered_title'].progress_apply(remove_chaos)\n",
    "df['filtered_description'] = df['filtered_description'].progress_apply(remove_chaos)\n",
    "any(i is None for i in df['filtered_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec81d570-872e-4075-8394-d5eac25f9ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet('../processed_data/tokenized_data.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c04e3d-da11-411e-b76f-fc45a5220af9",
   "metadata": {},
   "source": [
    "# Generate document-term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527c0cca-a7cb-4050-b44d-81fce7b2f8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "dtm = vectorizer.fit_transform(sum(df['clean_tok'], []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dce2cb-a4a0-44fc-b20a-f29fd9b79304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_pickle(obj, file_name):\n",
    "    import pickle\n",
    "    with open(file_name, 'wb') as fout:\n",
    "        pickle.dump(obj, fout, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "to_pickle(dtm, '../processed_data/dtm_sparse_mat.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b2e862-68d6-42df-a210-bb511c5eae2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_sampled = pd.DataFrame(dtm[:20].toarray(), columns=vectorizer.get_feature_names_out())\n",
    "dtm_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac313f0-63a0-4e07-9083-2736bb3f6a53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
