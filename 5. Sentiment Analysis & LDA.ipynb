{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c231440d-7357-407e-99a6-d3bf6641fb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.read_parquet('../processed_data/final_data_2015-23')\n",
    "for i in ['clean_tok']:\n",
    "    df[i] = df[i].apply(lambda f: f.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e99217-428d-4179-bb6d-d3f27349629c",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b497dfb6-2ba5-4f7a-8680-de03ac5ef5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove more chaos by checking word frequency \n",
    "words_des = sum(df['clean_tok'], [])\n",
    "from collections import Counter\n",
    "ct = Counter(words_des)\n",
    "sorted_ct = ct.most_common()\n",
    "for word, count in sorted_ct:\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27fb3b1-9c60-4b65-9591-c11590cb4c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "chaos = ['【', '】', '...', '）', '（', '#', '[', ']', '！！！', ',', '——', '-', '！！', '*', '.', '……', '+',\n",
    "        '!', ':', '/', '--', '|', '(', '~', '…', '—', ')', '～', '？？？', ';', '·', '？？', '。。。', '。。'\n",
    "        '！！！！', '---', '「', '」', 'quot', '..', '●', '**', '▎', '....', '😜' ]\n",
    "\n",
    "def remove_chaos(word_list):\n",
    "    clean_words = []\n",
    "    for word in word_list:\n",
    "        if word not in chaos:\n",
    "            clean_words.append(word)\n",
    "    return clean_words\n",
    "\n",
    "df['clean_tok'] = df['clean_tok'].apply(remove_chaos)\n",
    "any(i is None for i in df['clean_tok'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120e47b4-cca2-466f-b43f-eb33dcc8301f",
   "metadata": {},
   "source": [
    "## Sentiment Analysis using SnownLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706a50b7-c3d7-4361-af19-b1d44ce1fa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snownlp import SnowNLP\n",
    "from snownlp import sentiment\n",
    "def get_sentiment(text):\n",
    "    if len(text) == 0:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return SnowNLP(text).sentiments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ca9f8a-b4a3-4254-9e05-a8d406432714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate by joining tokens back together\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "df['joined_tok'] = df['clean_tok'].progress_apply(lambda tokens: ''.join(tokens))\n",
    "df['sentiment'] = df['joined_tok'].progress_apply(get_sentiment)\n",
    "df['sentiment'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8b4d00-74e7-4ebb-a64b-c41461a0a1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet('../processed_data/with_sentiment.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc735581-cb31-4880-8a3b-112e54991e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check which of \"filtered_title\" are empaty \n",
    "df['t_empty'] = df['filtered_title'].apply(lambda x: len(x) == 0)\n",
    "print(df['t_empty'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fd729b-f437-4ef7-9219-82a6b7910dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((df['sentiment']>0.5).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4335ac-dfdf-4df1-9c53-15a815e9b630",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.histplot(df['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7494233-c608-40a8-8ef7-b7b093e737a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((df['sentiment']<0.2).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb06fc9-9deb-4c7f-9c01-9674d69ecb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check top word counts with sentiment < 0.2\n",
    "from collections import Counter\n",
    "neg_stmt = df[df['sentiment']<0.2]\n",
    "neg_stmt_words = sum(neg_stmt['clean_tok'], [])\n",
    "ct_neg_words = Counter(neg_stmt_words)\n",
    "print(ct_neg_words.most_common(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d0d8c4-73a1-4093-a9b2-c112d97bf306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# average (or median) sentiment over years\n",
    "# df['year'] = df['date'].apply(lambda x: x[:4])\n",
    "# df['year'] = pd.to_numeric(df['year'])\n",
    "\n",
    "yearly_mean_sentiment = {}\n",
    "for year in sorted(df['year'].unique()):\n",
    "    df_year = df[df['year'] == year]\n",
    "    mean_stmt_year = df_year['sentiment'].mean()\n",
    "    yearly_mean_sentiment[year] = mean_stmt_year\n",
    "\n",
    "print(yearly_mean_sentiment)\n",
    "\n",
    "# Plot the results\n",
    "years = list(yearly_mean_sentiment.keys())\n",
    "mean_sentiments = list(yearly_mean_sentiment.values())\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(years, mean_sentiments, marker='o')\n",
    "plt.title('Mean Sentiment Over Years')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Mean Sentiment')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002c81fa-0102-40fc-b527-47e0cfda3a3f",
   "metadata": {},
   "source": [
    "## Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9820cf-5336-41c0-b84b-560824c8a8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate document-term matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_df=0.9, min_df=2, tokenizer=lambda x: x, preprocessor=lambda x: x)\n",
    "dtm = cv.fit_transform(df['clean_tok'])\n",
    "dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60af537e-1b6f-4ae4-89a5-6693f585c55a",
   "metadata": {},
   "source": [
    "### LDA in Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd127a63-6963-4631-9439-645eecea4452",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "LDA = LatentDirichletAllocation(n_components=10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ffe474-5419-4596-b8c6-194474d23341",
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA.fit(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7011d7-045d-43dc-8e4b-38867ac91587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the volucabulary of words\n",
    "cv.get_feature_names_out()[10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6441e9df-c0c7-44e6-9075-30d36aef9907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the topics\n",
    "LDA.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae5f2e9-3a3e-4c17-8dd1-0db810a44f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the highest probability words per topic\n",
    "single_topic = LDA.components_[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f85acb-6f2a-47b1-a6f7-bc80be680c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Argsort --> index positions sorted from least to greast\n",
    "single_topic.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462e36a8-7929-4691-9ffc-588b92a97533",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_twenty_words = single_topic.argsort()[-20:] # grad the last 20 values  .argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fae5e8-8ebd-40c5-89de-0bd54220b3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in top_twenty_words:\n",
    "    print(cv.get_feature_names_out()[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6860e484-d319-4c27-8758-6d17b4b50654",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,7):\n",
    "    print(f\"The top 15 words for topic #{i}\")\n",
    "    for index in LDA.components_[i].argsort()[-20:]:\n",
    "        print(cv.get_feature_names_out()[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffab2055-2f4c-4870-a5ca-8f0c0d294ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_results = LDA.transform(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc3eada-2a43-474b-aea7-ffb23866cb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_results.round(2) #shows probability of each topic for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0c37d4-f9f7-4567-bcfc-e0aa99053177",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d19ed8-0730-489f-9dee-35dc5abd0b97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate a column to assign topics to the dataframe\n",
    "df['topic_lda'] = topic_results.argmax(axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26831ef0-7735-48cd-bc72-ae5f742f0976",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df.iloc[1]['fulltext'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4705a46f-ddcd-4456-9ca2-0240d8bea798",
   "metadata": {},
   "source": [
    "### LDA using Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe7840d-960a-449e-8bff-f1de228e8000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code learned from: https://zhuanlan.zhihu.com/p/133779883"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1b3bcf-5558-4113-b421-ba0811df983b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfe8a49-b8c2-47d5-a36e-8763138a5bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim import corpora, models, similarities\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d1079b-9f0b-4a9c-be2b-7fefdb5cad60",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(df['clean_tok'])\n",
    "print(id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411cbca2-f06f-4ba2-88f0-efb89d859452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将字典转换为词袋,为文档中的每一个单词创建唯一的ID\n",
    "corpus = [id2word.doc2bow(token) for token in df['clean_tok']]\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c137c10-42d3-490a-85a5-d9acd1f929fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可通过如下预期查询id对应的词\n",
    "print([[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298c8514-5b73-4bb4-bd99-e039df0bea36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立LDA模型\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                id2word=id2word,\n",
    "                num_topics=10,\n",
    "                random_state=100,\n",
    "                update_every=1,\n",
    "                chunksize=100,\n",
    "                passes=10,\n",
    "                alpha='auto',\n",
    "                per_word_topics=True\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1680fdfc-c417-41f4-90aa-f9775dfc16b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上述LDA模型由10个不同的主题构建，其中每个主题是关键字的组合，并且每个关键字对主题贡献一定的权重，权重反应了关键字对主体的贡献程度。\n",
    "# num_word代表每个主题的关键字数\n",
    "pprint(lda_model.print_topics(num_words=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20524bd-8310-4940-8db9-9a32ade66893",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "模型复杂度和主题一致性提供了一种方便的方法来判断给定主题模型的好坏程度。\n",
    "特别是主题一致性得分更有帮助。\n",
    "'''\n",
    "# def model():\n",
    "# Compute Perplexity 模型复杂度\n",
    "print('Perplexity: ', lda_model.log_perplexity(corpus)) # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=df['clean_tok'], dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('Coherence Score: ', coherence_lda) # 主题一致性得分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2afe16-b45b-427c-96f5-a0f8a41cf81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 寻找LDA的最佳主题数\n",
    "coherence_values = []\n",
    "model_list = []\n",
    "for num_topics in range(2,15,1):\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                    id2word=id2word,\n",
    "                    num_topics=num_topics,\n",
    "                    random_state=100,\n",
    "                    update_every=1,\n",
    "                    chunksize=200,\n",
    "                    passes=20,\n",
    "                    alpha='asymmetric',\n",
    "                    per_word_topics=True\n",
    ")\n",
    "    model_list.append(lda_model)\n",
    "    coherencemodel = CoherenceModel(model=lda_model, texts=df['clean_tok'], dictionary=id2word, coherence='c_v')\n",
    "    coherence_values.append(round(coherencemodel.get_coherence(),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ddbd65-3db1-43bf-bd97-137498b60081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最佳主题数可视化展示\n",
    "x = range(2,15,1)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.savefig('../Graphs/LDA_coherence_scores')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abc59a2-7b61-4c69-9451-5efcfb5e6cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 展示不同的主题数对应的一致性分数\n",
    "for m, cv in zip(x, coherence_values):\n",
    "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c745abf-9ab0-4ca0-ae9c-4fa66aae050b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据一致性得分，选择具有最高CV的模型，选择的主题数是\n",
    "# 选择模型并打印主题   \n",
    "optimal_model = model_list[8]\n",
    "model_topics = optimal_model.show_topics(formatted=False)\n",
    "pprint(optimal_model.print_topics(num_words=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea76888-3403-47fd-bc40-0735f4e106f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------在每个句子中找到主要话题--------------------------------------------\n",
    "# 每个文档都包含多个主题。但是，通常只有一个主题是主导的。下面的代码为每个文档提取该主要主题，并在格式正确的输出中显示该主题的权重。\n",
    "def format_topics_sentences(ldamodel, corpus, texts):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row[0], key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0: # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                new_row = pd.DataFrame([[int(topic_num), round(prop_topic,4), topic_keywords]],\n",
    "                       columns=['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords'])\n",
    "                sent_topics_df = pd.concat([sent_topics_df, new_row], ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    # print(contents)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    # print(sent_topics_df)\n",
    "    return(sent_topics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5d8c02-cd27-4b93-8960-01ea4b7c86f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=df['fulltext'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db137eb6-691b-483c-9032-5bcb7e5fbf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_sents_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6371f3f-f76e-4968-9989-d48e71768619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# Show\n",
    "# df_dominant_topic.to_excel(path+'resultsdatas.xlsx',index=False)\n",
    "df_dominant_topic.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5de344-ec88-49a7-9d7d-076495001fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------每个主题中最具有代表性的句子-----------------------------------------\n",
    "# Display setting to show more characters in column\n",
    "pd.options.display.max_colwidth = 100\n",
    "\n",
    "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet,\n",
    "grp.sort_values(['Perc_Contribution'], ascending=False).head(2)],\n",
    "axis=0)\n",
    "\n",
    "# Reset Index\n",
    "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Representative Text\"]\n",
    "\n",
    "# Show\n",
    "sent_topics_sorteddf_mallet.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d265c921-bd81-4456-9b28-cebec64c7145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文件中字数的频率分布\n",
    "\n",
    "df_dominant_topic = df_dominant_topic.dropna(axis=0)\n",
    "doc_lens = [len(d) for d in df_dominant_topic.Text]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(16,7), dpi=160)\n",
    "plt.hist(doc_lens, bins = 1000, color='navy')\n",
    "plt.text(750, 100, \"Mean : \" + str(round(np.mean(doc_lens))))\n",
    "plt.text(750, 90, \"Median : \" + str(round(np.median(doc_lens))))\n",
    "plt.text(750, 80, \"Stdev : \" + str(round(np.std(doc_lens))))\n",
    "plt.text(750, 70, \"1%ile : \" + str(round(np.quantile(doc_lens, q=0.01))))\n",
    "plt.text(750, 60, \"99%ile : \" + str(round(np.quantile(doc_lens, q=0.99))))\n",
    "\n",
    "plt.gca().set(xlim=(0, 1000), ylabel='Number of Documents', xlabel='Document Word Count')\n",
    "plt.tick_params(size=16)\n",
    "plt.xticks(np.linspace(0,1000,9))\n",
    "plt.title('Distribution of Document Word Counts', fontdict=dict(size=22))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb7b33b-b702-4a66-9d8e-dac6650fb134",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.colors as mcolors\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()] # more colors: 'mcolors.XKCD_COLORS'\n",
    "\n",
    "fig, axes = plt.subplots(2,2,figsize=(16,14), dpi=160, sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    df_dominant_topic_sub = df_dominant_topic.loc[df_dominant_topic.Dominant_Topic == i, :]\n",
    "    doc_lens = [len(d) for d in df_dominant_topic_sub.Text]\n",
    "    ax.hist(doc_lens, bins = 1000, color=cols[i])\n",
    "    ax.tick_params(axis='y', labelcolor=cols[i], color=cols[i])\n",
    "    sns.kdeplot(doc_lens, color=\"black\", shade=False, ax=ax.twinx())\n",
    "    ax.set(xlim=(0, 1000), xlabel='Document Word Count')\n",
    "    ax.set_ylabel('Number of Documents', color=cols[i])\n",
    "    ax.set_title('Topic: '+str(i), fontdict=dict(size=16, color=cols[i]))\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.90)\n",
    "plt.xticks(np.linspace(0,1000,9))\n",
    "fig.suptitle('Distribution of Document Word Counts by Dominant Topic', fontsize=22)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aced8cd6-8a56-4e62-acac-68d1a7dffc6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
